{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3d31476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "# hint: sometimes execution break becuase e.g. you accidentally press something in the opened browser or connection issues or smth similar:\n",
    "# normally restring the kernal and rerunning a particular period where it broke solves the issue\n",
    "# normally I left this runnning overnight, it takes several hours to scrape 2007 - 2023\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e96b3728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download the ChromeDriver from the official website.\n",
    "# Go to https://chromedriver.chromium.org/downloads and download the appropriate version for your operating system.\n",
    "\n",
    "# Step 2: Adjust security settings (Mac specific).\n",
    "# On Mac, you may encounter an error stating that the file cannot be opened due to security settings.\n",
    "# To fix this, follow these steps:\n",
    "#   - Open \"System Preferences\" from the Apple menu.\n",
    "#   - Click on \"Security & Privacy.\"\n",
    "#   - In the \"General\" tab, you should see a message saying \"chromedriver\" was blocked from opening.\n",
    "#   - Click the \"Open Anyway\" button next to the message.\n",
    "#   - You might be prompted to enter your username and password.\n",
    "#   - After allowing the file to open, continue with the steps below.\n",
    "\n",
    "# Step 3: Set the path to the ChromeDriver executable.\n",
    "# Replace the PATH variable below with the path to the ChromeDriver executable on your machine.\n",
    "# Make sure to include the full path, including the folder and filename of the ChromeDriver.\n",
    "\n",
    "\n",
    "PATH = r\"/Users/juliaprozorova/Downloads/chromedriver_mac64-3/chromedriver\"  # Replace with your own path\n",
    "\n",
    "# Step 4: Create a WebDriver instance using ChromeDriver.\n",
    "# The following code initializes the ChromeDriver with the provided path.\n",
    "# It will allow you to automate Google Chrome using Selenium.\n",
    "\n",
    "driver = webdriver.Chrome(PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c4ceb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function which is later used in scraping\n",
    "\n",
    "def tick_courses (table, choice, a = None, b=None):\n",
    "     \"\"\"\n",
    "     Tick Courses Function:\n",
    "     Selects specific courses on a web page using Selenium WebDriver.\n",
    "\n",
    "     Parameters:\n",
    "     - table (WebElement): The table or container element containing the course options.\n",
    "     - choice (str): The choice of courses to select. If \"Faculty of Humanities and Social Sciences\",\n",
    "                            additional logic will be applied due to different page rendering.\n",
    "     - a (str): Optional parameter. Text to exclude from selection if \"Faculty of Humanities and Social Sciences\" is chosen.\n",
    "     - b (str): Optional parameter. Text to exclude from selection if \"Faculty of Humanities and Social Sciences\" is chosen.\n",
    "     \"\"\"\n",
    "     if choice == \"Faculty of Humanities and Social Sciences\":\n",
    "            found1 = False\n",
    "            found2 = False\n",
    "            found = False\n",
    "\n",
    "            for el in table.find_elements(By.TAG_NAME,\"li\"):\n",
    "                for e in reversed(el.find_elements(By.TAG_NAME,\"span\")):\n",
    "                    \n",
    "                    if e.get_attribute(\"class\") == \"fancytree-title\":\n",
    "                          if e.text == \"Faculty of Humanities and Social Sciences\":\n",
    "                            found1 = True\n",
    "                            found2 = True\n",
    "\n",
    "                    elif found1 and e.get_attribute(\"class\") == \"fancytree-expander\":\n",
    "                        webdriver.ActionChains(driver).move_to_element(e).click().perform()\n",
    "                        found1 = False\n",
    "                        \n",
    "   \n",
    "                    elif found2 and e.get_attribute(\"class\") == \"fancytree-checkbox\":\n",
    "                        webdriver.ActionChains(driver).move_to_element(e).click().perform()\n",
    "                        found2 = False\n",
    "                                  \n",
    "                                            \n",
    "            done = 0\n",
    "            for el in table.find_elements(By.TAG_NAME,\"li\"):\n",
    "                for e in reversed(el.find_elements(By.TAG_NAME,\"span\")):\n",
    "                    if e.get_attribute(\"class\") == \"fancytree-title\":\n",
    "         \n",
    "                        if a in e.text  or b in e.text : \n",
    "\n",
    "                            found = True\n",
    "                           \n",
    "\n",
    "                    elif done < 2 and found and e.get_attribute(\"class\") == \"fancytree-checkbox\":\n",
    "                        webdriver.ActionChains(driver).move_to_element(e).click().perform()\n",
    "                        found = False\n",
    "                        done+=1\n",
    "                        \n",
    "     else:\n",
    "            found = False\n",
    "            for el in table.find_elements(By.TAG_NAME,\"li\"):\n",
    "                for e in reversed(el.find_elements(By.TAG_NAME,\"span\")):\n",
    "                    if e.get_attribute(\"class\") == \"fancytree-title\":\n",
    "                        if e.text == choice:\n",
    "                            found = True\n",
    "                    elif found and e.get_attribute(\"class\") == \"fancytree-checkbox\":\n",
    "                        webdriver.ActionChains(driver).move_to_element(e).click().perform()\n",
    "                        found = False\n",
    "                        \n",
    "     apply = driver.find_elements_by_xpath(\"//*[contains(text(), 'Apply')]\")\n",
    "     webdriver.ActionChains(driver).move_to_element(apply[0]).click().perform()\n",
    "     \n",
    "     # unfreeze\n",
    "     driver.find_element_by_xpath(\"//body\").click() # apply =  driver.find_element(By.CLASS_NAME, \"col-lg-12\")\n",
    "     #webdriver.ActionChains(driver).move_to_element(apply).click().perform()\n",
    "    \n",
    "     #make sure it was pressed properly\n",
    "     apply = driver.find_elements_by_xpath(\"//*[contains(text(), 'Show results')]\")\n",
    "     webdriver.ActionChains(driver).move_to_element(apply[0]).click().perform()\n",
    "     time.sleep(20) \n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c879b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_list(m, a = None, b = None, additional_name = \"\"):\n",
    "            \"\"\"\n",
    "            Scrape List Function:\n",
    "            Scrapes a list of courses from a web page using Selenium WebDriver and saves the results to a CSV file.\n",
    "\n",
    "            Parameters:\n",
    "                - m (str): The choice of courses to select.\n",
    "                - a (str): Optional parameter. Text to exclude from selection if \"Faculty of Humanities and Social Sciences\" is chosen.\n",
    "                - b (str): Optional parameter. Text to exclude from selection if \"Faculty of Humanities and Social Sciences\" is chosen.\n",
    "                - additional_name (str): Optional parameter. The name to be included in the resulting CSV filename.\n",
    "\n",
    "            Returns:\n",
    "                None. The function saves the scraped course data to a CSV file.\n",
    "\n",
    "            \"\"\"\n",
    "    # first open, unfreeze, then press reset\n",
    "            el = driver.find_elements_by_xpath(\"//*[contains(text(), 'Program structure')]\")\n",
    "            webdriver.ActionChains(driver).move_to_element(el[1]).click().perform()\n",
    "            time.sleep(1)\n",
    "            #p = driver.find_element(By.TAG_NAME,\"p\"); \n",
    "            #webdriver.ActionChains(driver).move_to_element(p).click().perform()\n",
    "            apply = driver.find_elements_by_xpath(\"//*[contains(text(), 'Reset')]\")\n",
    "            webdriver.ActionChains(driver).move_to_element(apply[2]).click().perform()\n",
    "\n",
    "            #open from the beginning\n",
    "            x = \"https://vorlesungsverzeichnis.unibas.ch/en/investigation?periode=2021005&search=1\"\n",
    "            x = x.replace(\"2021005\", semester)\n",
    "            driver.get(x)\n",
    "            # adjust settings\n",
    "            el = driver.find_elements_by_xpath(\"//*[contains(text(), 'Program structure')]\")\n",
    "            webdriver.ActionChains(driver).move_to_element(el[1]).click().perform()\n",
    "            time.sleep(1)\n",
    "            table = driver.find_element(By.XPATH, \"//ul[@class='ui-fancytree fancytree-container fancytree-plain']\")\n",
    "\n",
    "\n",
    "            tick_courses(table, m, a, b)\n",
    "\n",
    "\n",
    "            # construct database\n",
    "            columns = [\"index\", \"name\", \"link\"]\n",
    "            df = pd.DataFrame(columns =columns)      \n",
    "\n",
    "            try:\n",
    "                table = driver.find_element(By.XPATH, \"//table[@class='table table-striped table-bordered table-hover dataTable no-footer']\")\n",
    "                print(table)\n",
    "                while True:\n",
    "                    try: \n",
    "                        rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                        id_ = rows[1].find_elements(By.TAG_NAME, \"td\")[1].text\n",
    "                        if id_ in set(df[\"index\"]):\n",
    "                                    print(\"Duplicate entry encountered. Everything is processed. Exiting loop.\")\n",
    "                                    break\n",
    "\n",
    "                        k = 0\n",
    "                        for row in rows:\n",
    "                            if k >0:   \n",
    "                                    id_ = row.find_elements(By.TAG_NAME, \"td\")[1].text\n",
    "                                    name = row.find_elements(By.TAG_NAME, \"td\")[3].text\n",
    "                                    link = row.find_elements(By.TAG_NAME, \"td\")[3].find_elements(By.TAG_NAME, \"a\")[0].get_attribute('href')\n",
    "                                    row = [id_, name, link]\n",
    "                                    df.loc[len(df.index)] = row\n",
    "                            k +=1\n",
    "                        next_page = driver.find_elements_by_xpath(\"//*[contains(text(), 'Next')]\")\n",
    "                        webdriver.ActionChains(driver).move_to_element(next_page[0]).click().perform()       \n",
    "\n",
    "                    except Exception as e: \n",
    "                        print(\"Error occurred while scraping the course list. Exiting loop.\", x, e)\n",
    "                        break\n",
    "                # save the result, adjust the path where you want to save the resulting csv\n",
    "                df.to_csv('test/courses_%s_%s_%s.csv' %(semester, m, additional_name),  encoding='utf-8', index=False) #fixme \n",
    "                print(\"Scraping complete. Data saved to courses_%s_%s_%s.csv\" %(semester, m, additional_name))\n",
    "            except Exception as e: \n",
    "                    print(\"An exception occurred during scraping:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bd170292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<selenium.webdriver.remote.webelement.WebElement (session=\"79bcb79a92184577b4f6d1a76f5d13c5\", element=\"16D7A8CBEA9EAF9985D16BDD4C441A4E_element_35130\")>\n",
      "Duplicate entry encountered. Everything is processed. Exiting loop.\n",
      "Scraping complete. Data saved to courses_2007004_University of Basel_.csv\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"79bcb79a92184577b4f6d1a76f5d13c5\", element=\"BA7BF559A5681973FF6E2E78036D6F7F_element_36710\")>\n",
      "Error occurred while scraping the course list. Exiting loop. https://vorlesungsverzeichnis.unibas.ch/en/investigation?periode=2007004&search=1 list index out of range\n",
      "Scraping complete. Data saved to courses_2007004_Faculty of Theology_.csv\n",
      "An exception occurred during scraping: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//table[@class='table table-striped table-bordered table-hover dataTable no-footer']\"}\n",
      "  (Session info: chrome=114.0.5735.133)\n",
      "\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"79bcb79a92184577b4f6d1a76f5d13c5\", element=\"A2B47BFD4DDC7F6542148681D7068DE5_element_36906\")>\n",
      "Duplicate entry encountered. Everything is processed. Exiting loop.\n",
      "Scraping complete. Data saved to courses_2007004_Faculty of Medicine_.csv\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"79bcb79a92184577b4f6d1a76f5d13c5\", element=\"D8C47658BD4D6313BFE5B88A4BC19494_element_38746\")>\n",
      "Duplicate entry encountered. Everything is processed. Exiting loop.\n",
      "Scraping complete. Data saved to courses_2007004_Faculty of Humanities and Social Sciences_Doktorat.csv\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"79bcb79a92184577b4f6d1a76f5d13c5\", element=\"E074546E3BC361BA54CC12B6741D6708_element_39361\")>\n",
      "Duplicate entry encountered. Everything is processed. Exiting loop.\n",
      "Scraping complete. Data saved to courses_2007004_Faculty of Humanities and Social Sciences_Master.csv\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"79bcb79a92184577b4f6d1a76f5d13c5\", element=\"6D2F1CC5185FA7E1A17FA827BD6DDC0B_element_43857\")>\n",
      "Duplicate entry encountered. Everything is processed. Exiting loop.\n",
      "Scraping complete. Data saved to courses_2007004_Faculty of Humanities and Social Sciences_Bachelor.csv\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"79bcb79a92184577b4f6d1a76f5d13c5\", element=\"96585E2DA4DC0A044CBBE0EAD8DD7EEF_element_51823\")>\n",
      "Duplicate entry encountered. Everything is processed. Exiting loop.\n",
      "Scraping complete. Data saved to courses_2007004_Faculty of Science_.csv\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"79bcb79a92184577b4f6d1a76f5d13c5\", element=\"0B1855C1F645DBCFD33C95D5EEBFDD20_element_58217\")>\n",
      "Duplicate entry encountered. Everything is processed. Exiting loop.\n",
      "Scraping complete. Data saved to courses_2007004_Faculty of Business and Economics_.csv\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"79bcb79a92184577b4f6d1a76f5d13c5\", element=\"02EDFA50F0E2528073F2DA0F7A7C89B7_element_58847\")>\n",
      "Duplicate entry encountered. Everything is processed. Exiting loop.\n",
      "Scraping complete. Data saved to courses_2007004_Faculty of Psychology_.csv\n",
      "<selenium.webdriver.remote.webelement.WebElement (session=\"79bcb79a92184577b4f6d1a76f5d13c5\", element=\"17D612E3466CBDA67032CA9DB2F9DE22_element_59455\")>\n",
      "Duplicate entry encountered. Everything is processed. Exiting loop.\n",
      "Scraping complete. Data saved to courses_2007004_Teacher training program_.csv\n"
     ]
    }
   ],
   "source": [
    "# This script performs the first step of the scraping process to retrieve the \"index,\" \"name,\" and \"link\" for each course from the University course catalog. \n",
    "#It iterates over different semesters and faculties, navigates to the respective pages, \n",
    "#and extracts the course information. The script utilizes Selenium and BeautifulSoup to interact with the web pages and parse the HTML content. \n",
    "#The extracted data is stored in CSV files for further processing in the second step. \n",
    "#For the \"Faculty of Humanities and Social Sciences,\" the script retrieves course information for different combinations of \"Bachelor,\" \"Master,\" and \"Doktorat\" programs.\n",
    "#This is because simulatanous processsing of all results in error due to large number of courses\n",
    "\n",
    "for year in range (2007, 2008):\n",
    "    for period in range (4, 5):\n",
    "        semester = ind = str(year)+(\"00\")+str(period)\n",
    "\n",
    "        \n",
    "        # open a page once for each semester\n",
    "        x = \"https://vorlesungsverzeichnis.unibas.ch/en/investigation?periode=2021005&search=1\"\n",
    "        x = x.replace(\"2021005\", semester)\n",
    "        driver.get(x)\n",
    "\n",
    "        # constructing the list of all the faculties\n",
    "        mylist = []\n",
    "        el = driver.find_elements_by_xpath(\"//*[contains(text(), 'Program structure')]\")\n",
    "        webdriver.ActionChains(driver).move_to_element(el[1]).click().perform()\n",
    "        time.sleep(1)\n",
    "        table = driver.find_element(By.XPATH, \"//ul[@class='ui-fancytree fancytree-container fancytree-plain']\")\n",
    "        for el in table.find_elements(By.TAG_NAME,\"li\"):\n",
    "                    for e in reversed(el.find_elements(By.TAG_NAME,\"span\")):\n",
    "                        if e.get_attribute(\"class\") == \"fancytree-title\":\n",
    "                            mylist.append(e.text)\n",
    "\n",
    "        x = \"https://vorlesungsverzeichnis.unibas.ch/en/investigation?periode=2021005&search=1\"\n",
    "        x = x.replace(\"2021005\", semester)\n",
    "        driver.get(x)  \n",
    "\n",
    "        for m in mylist:\n",
    "            if m == \"Faculty of Humanities and Social Sciences\":\n",
    "                for (a, b) in combinations([\"Bachelor\", \"Master\", \"Doktorat\"], 2):\n",
    "                    additional_name = (set([\"Bachelor\", \"Master\", \"Doktorat\"]) - set([a, b])).pop()\n",
    "                    scrape_list(m, a, b, additional_name)\n",
    "            else:\n",
    "                scrape_list(m)\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c19fb0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "courses_2007004_Teacher training program_.csv 65\n",
      "courses_2007004_Faculty of Theology_.csv 0\n",
      "An exception occurred while processing file: courses_2007004_Faculty of Theology_.csv 'NoneType' object has no attribute 'to_csv'\n",
      "courses_2007004_Faculty of Psychology_.csv 42\n",
      "courses_2007004_Faculty of Humanities and Social Sciences_Master.csv 385\n",
      "An exception occurred while processing file: scraped [Errno 21] Is a directory: 'test/scraped'\n",
      "courses_2007004_Faculty of Science_.csv 556\n",
      "courses_2007004_University of Basel_.csv 128\n",
      "courses_2007004_Faculty of Humanities and Social Sciences_Doktorat.csv 40\n",
      "An exception occurred while processing file: .ipynb_checkpoints [Errno 21] Is a directory: 'test/.ipynb_checkpoints'\n",
      "courses_2007004_Faculty of Business and Economics_.csv 45\n",
      "courses_2007004_Faculty of Medicine_.csv 149\n",
      "courses_2007004_Faculty of Humanities and Social Sciences_Bachelor.csv 695\n"
     ]
    }
   ],
   "source": [
    "# This script performs the second step of the scraping process \n",
    "#to retrieve detailed data for the courses obtained in the first step. \n",
    "#It reads CSV files containing the course information, scrapes the individual course pages to extract additional details, \n",
    "#and saves the combined data into the same CSV files. The script uses BeautifulSoup to parse the HTML content of each course page and retrieves specific elements. \n",
    "#The extracted data is then structured into a DataFrame and concatenated with the existing course data. \n",
    "#The resulting DataFrame is saved back to the CSV files. \n",
    "#The script also handles exceptions to ensure smooth processing of multiple files.\n",
    "\n",
    "# adjust the path to where you saved the results from the first step\n",
    "dirr = os.getcwd()+\"/test\" #fixme\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(dirr):\n",
    "                try:\n",
    "                    # Read the CSV file containing the scraped course data\n",
    "                    df = pd.read_csv('test/'+filename ) #fixme\n",
    "                    print(filename, len(df)) # manual step: cross check with the website that the number of courses is correct and everyhting got sraped\n",
    "                    # Get the links to each course page\n",
    "                    links = list(df[\"link\"])\n",
    "                    # Initialize the final data DataFrame\n",
    "                    final_data = None\n",
    "                    # Iterate over each course link\n",
    "                    for link in links:\n",
    "                        #link =  link.replace(\"en/semester-planning\", \"de/semester-planung\")\n",
    "                        driver.get(link)\n",
    "                        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                        data = pd.DataFrame(columns =[\"course_name\", link])\n",
    "                        # Find the course information using BeautifulSoup\n",
    "                        s = soup.findAll(\"strong\")\n",
    "                        for el in s:\n",
    "                            new = soup.find(\"td\", text=el.text).find_next_sibling(\"td\").text\n",
    "                            row = [el.text, new]\n",
    "                            data.loc[len(data)]=row\n",
    "                        data.set_index('course_name',inplace=True)\n",
    "                        data = data.transpose()\n",
    "                        for el in soup.findAll(\"li\"):\n",
    "                            if \" CP\" in el.text:\n",
    "                                data[\"full_name\"] = el.text\n",
    "                        # Concatenate the course data to the final_data DataFrame\n",
    "                        if final_data is not None:\n",
    "                            final_data = pd.concat([final_data, data], ignore_index=False)\n",
    "                        else:\n",
    "                            final_data = data\n",
    "                     # Save the final_data DataFrame to a CSV file\n",
    "                    final_data.to_csv('test/scraped/'+filename,  encoding='utf-8', index=False)#fixme\n",
    "                except Exception as e:\n",
    "                    print(\"An exception occurred while processing file:\", filename, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c9d96d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
